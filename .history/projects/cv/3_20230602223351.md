## Mutual Learning Approach to Distracted Driver Behavior Detection

**Project description:** Distracted driver behavior recognition aims to prevent accidents by alerting drivers of distracted driving. This study investigates the potential mutual learning between specific-object detectors to improve accuracy by combining their results to achieve a comprehensive understanding of the driver's cabin space. The recognized scene is then used to reevaluate the predicted objects and their locations within the cabin, such as the driver's face, hands, steering wheel, cigarette, handphone, sandwich, coffee mug, and more.

### 1. Learning architecture of the system

The upper part of the model
The behavior is predicted using the entire image, while the lower part of the model employs specific object detectors. The face detector results are represented by yellow bounding boxes, the hands detector results by green bounding boxes, and the steering detector result by a blue bounding box. By combining the outputs of individual object detectors, a scene prediction is generated. The image classifier and ensemble of object detectors learn collaboratively, each with its own supervised learning loss. Additionally, a mimicry loss based on Kullback-Leibler Divergence is employed to align the probability estimates of the different components (Y. Zhang et al., 2018).

<img src="images/cv/project_2/interface.png?raw=true"/>

### 2. Dataset used

This study utilized a subset of the 3MDAD dataset (Jegham et al., 2020) for training and testing the model. The side-view set of the dataset was chosen as it contains bounding boxes for the face and hands. For training, images of 30 subjects were used, while the images of 10 subjects were used for testing. The training set consisted of 65,003 frames, while the testing set had 19,542 frames. The distribution of frames among the classes can be seen in Figure C.1.

The app currently employs conventional computer vision techniques to accomplish the following tasks:

Differentiate the foreground object (i.e., passenger) from the background.
Identify pixels that belong to the same foreground object.
These operations involve standard computer vision procedures such as erosion, dilation, background subtraction, and contour detection.

<img src="images/cv/project_2/detecting_passenger_1.gif?raw=true"/>
<img src="images/cv/project_2/detecting_passenger_2.gif?raw=true"/>

### 3. Recognizing the Passenger

The application quantifies the "color distribution" linked to an individual as they board the vehicle.

The application utilizes the "color distribution" to identify an individual as they disembark from the vehicle.

<img src="images/cv/project_2/recognizing_the_passenger_1.png?raw=true"/>

Subsequently, the application computes the "similarity" between the image of the departing passenger and each passenger present inside the vehicle.

The application then chooses the image that exhibits the highest level of similarity.

<img src="images/cv/project_2/recognizing_the_passenger_2.png?raw=true"/>
