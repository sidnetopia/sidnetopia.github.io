## Mutual Learning Approach to Distracted Driver Behavior Detection

**Study description:** Distracted driver behavior recognition aims to prevent accidents by alerting drivers of distracted driving. This study investigates the potential mutual learning between specific-object detectors to improve accuracy by combining their results to achieve a comprehensive understanding of the driver's cabin space. The recognized scene is then used to reevaluate the predicted objects and their locations within the cabin, such as the driver's face, hands, steering wheel, cigarette, handphone, sandwich, coffee mug, and more.

### 1. Architecture of the Model

<img src="images/cv/project_3/overview_of_the_architecture.png?raw=true"/>

The behavior is predicted using the entire image, while the lower part of the model employs specific object detectors. The face detector results are represented by yellow bounding boxes, the hands detector results by green bounding boxes, and the steering detector result by a blue bounding box. By combining the outputs of individual object detectors, a scene prediction is generated. The image classifier and ensemble of object detectors learn collaboratively, each with its own supervised learning loss. Additionally, a mimicry loss based on Kullback-Leibler Divergence is employed to align the probability estimates of the different components [[1]](#1).

This architecture is inspired by R\* CNN [[2]](#2), a CNN model for action recognition that based its classification on region of interest. and SSD [[3]](#3) and YOLO NAS [[4]](#4) for object detection.

### 2. Dataset Used

This study utilized a subset of the 3MDAD dataset [[5]](#5) for training and testing the model. The dataset provides a diverse range of real-world detection challenges, including occlusions, blurry video frames, and variations in lighting and weather conditions. The side-view set of the dataset was selected, which includes bounding boxes for the face and hands.
<img src="images/cv/project_3/dataset_original.png?raw=true"/>

To further enhance the dataset, we supplemented it with additional annotations for phone, cigarette, drink, operational device, and steering wheel bounding boxes. For training, images from 30 subjects were used, while 10 subjects' images were allocated for validation and 10 for testing purposes.
<img src="images/cv/project_3/extended_annotations.png?raw=true"/>

### 3. Initial Results

#### Classification Accuracy evaluation

| Model                                            | Train  | Test   |
| ------------------------------------------------ | ------ | ------ |
| VGG-16                                           | 99.72% | 60.23% |
| Ground truth based network                       | 98.62% | 66.54% |
| Mutual Learning based VGG-16                     | 99.69% | 67.77% |
| Mutual Learning based ground truth based network | 99.93% | 68.85% |

#### Object Detection mAP evaluation

| Model    | Train | Test   |
| -------- | ----- | ------ |
| SSD      | 65%   | 67.77% |
| YOLO-NAS |       | 86.93% |

Based on the initial results, it is evident that the classifier benefits from mutual learning. The study is still ongoing, and I intend to explore the possibility of having the object detection module learn from the classifier, despite their distinct tasks.

---

### References

<a id="1">[1]</a>
Zhang, Y., Xiang, T., Hospedales, T. M., & Lu, H. (2018). _Deep mutual learning_. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 4320–4328).

<a id="2">[2]</a>
Gkioxari, G., Girshick, R., & Malik, J. (2015). _Contextual action recognition with R\*CNN_. In Proceedings of the IEEE International Conference on Computer Vision (pp. 1080-1088).

<a id="3">[3]</a>
Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C. Y., & Berg, A. C. (2016). _SSD: Single Shot MultiBox Detector_. In Computer Vision–ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11–14, 2016, Proceedings, Part I 14 (pp. 21-37). Springer International Publishing.

<a id="4">[4]</a>
Aharon, S., Louis-Dupont, & Masad, O., Yurkova, K., Lotem Fridman, Lkdci, Khvedchenya, E., Rubin, R., Bagrov, N., Tymchenko, B., Keren, T., Zhilko, A., & Eran-Deci. (2021). Super-Gradients. GitHub repository. Retrieved from https://zenodo.org/record/7789328

<a id="5">[5]</a>
Jegham, I., Khalifa, A. B., Alouani, I., & Mahjoub, M. A. (2020). A novel public dataset for multimodal multiview and multispectral driver distraction analysis: 3mdad. _Signal Processing: Image Communication_, 88, 115960.
