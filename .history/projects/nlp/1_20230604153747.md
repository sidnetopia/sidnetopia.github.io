## Predicting Stock Market Price Movement using NLP on Reddit Data

**Project description:** There have been a lot of discussions about the financial market in Reddit and WallStreetBets. Conversations on these possibly have the ability to influence the stock market. The goal of this project is to leverage the Reddit text data to predict a stock market movement. To utilize the text data, I employed sentence embedding, document embedding, and Neural Network (NN) models. Accordingly, this project examines several NN architecture and text data. Results show that text-based model can outperform the naive forecasting approach.

### Embeddings

I employed SBERT (Sentence-BERT) [[3]](#3) to convert textual data into sentence embeddings. I fine-tuned the model [[4]](#4) to produce embeddings with a dimensionality of 768. Additionally, I generated document embeddings of size 50 using Doc2Vec [[4]](#4), trained on the corpus.

### Architecture

<img src="images/nlp/1/architecture.png?raw=true"/>

The following modules were used on top of the embedding, as shown in Figure 1, to enhance the analysis:

-   **Sentence CNN Module**: This module takes the title as input and utilizes SBERT [[3]](#3) to generate sentence embedding vectors. These vectors are then fed into a CNN model to extract relevant information.

-   **Document CNN Module**: Instead of individual sentences, the body text is converted into document embeddings [[4]](#4), which are then used as inputs for the CNN model. This module operates similarly to the Sentence CNN Module.

-   **Title Mentions Module**: In addition to the title text data, I examined the impact of the number of title mentions on the model's accuracy. This module specifically focuses on this relationship.

-   **Body Mentions Module**: Similar to the Title Mentions CNN Module, this module investigates the correlation between body mentions and accuracy in the model's predictions.
    <img src="images/nlp/1/overview.png?raw=true"/>

### Exploratory Data Analysis

I conducted the following data processing and exploration procedures to facilitate model development:

-   **Data cleaning**: The collected data was cleaned by converting it to lowercase and removing web links, symbols, and stop words.

-   **Data exploration**: I compiled a list of US stocks and extracted the counts of mentions in both the title and body of the text. Specifically, I focused on stocks that were frequently mentioned in financial news related to Reddit stocks.

-   **Model target**: For the model target, I selected market movement, which is calculated as follows:
    <img src="images/nlp/1/movement_eq.png?raw=true"/>

Here, **x** represents **Open_t** - **Close_t** (the difference between the opening and closing prices), and **r** is the threshold (in this case, 0.003). The Movementt can be categorized as an up, neutral, or down movement. It is reasonable to consider the movement because small price changes may be insignificant due to higher transaction costs.

Overall, the summary of the data is as follows:

-   Assets chosen: Wipro Limited (WIT)
-   Time Range: January 01, 2020 to September 13, 2021
-   Number of posts: 1,395,676
-   WIT mentions: 111,427
-   Number of historical prices: 2,983
-   WIT text and price data: 240,138

### Baseline

I used the naive forecasting approach, which entails predicting future outcomes by relying on the present market price or prevailing conditions. In this project, the forecasting equation utilized is as follows:

<img src="images/nlp/1/baseline_eq.png?raw=true"/>

To ensure the effectiveness of the baseline, it is crucial for the WIT price and movement to exhibit a strong correlation with historical performance. The figure below demonstrates the autocorrelation and partial autocorrelation, clearly indicating a significant association between WIT price, movement, and past performance.

<img src="images/nlp/1/wit_time_series_analysis.png.png?raw=true"/>

### Experiment Results

The table below shows the accuracy of the train and test set. Here are some of my observations based on the results of the experiment:

-   There is an improvement in the accuracy with the inclusion of the number of mentions.
-   Sentence embedding performs better than document embedding, given that it is only based on the title. The reason for this is that document embeddings are mixed with different day comments, thus affecting the accuracy of the model. This is a constraint in the data collection as pushshift.io API converts the comments into a single text blob, making the different day comments undifferentiated.
-   All the models are underfitted. The cause of this is the architecture of the models. Since the models only have 32 filters, they do not capture the whole vector embeddings.
-   The models of this project achieved lower accuracy compared to the results of [1]. However, this is for a single stock. The reported accuracy of the main reference paper is the stocks mentioned per day. Also, this is for the case of a down-sampled dataset.

| Method                                 | Training Period | Testing Period |
| -------------------------------------- | --------------- | -------------- |
| Baseline                               | 39%             | 37.4%          |
| Sentence NN Model                      | 49%             | 47%            |
| Document NN Model                      | 47%             | 44.6%          |
| Sentence + Mention NN Model            | 49%             | 51%            |
| Document + Mention NN Model            | 50.5%           | 46.7%          |
| Sentence + Document + Mention NN Model | 40.6%           | 38%            |

### References

<a id="1">[1]</a>
M. Xu, "NLP for Stock Market Prediction with Reddit Data," February 2021.

<a id="3">[3]</a>
N. Reimers and I. Gurevych, "Sentence-BERT: Sentence EMBEDDINGS using siamese bert-networks," Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 2019.

<a id="4">[4]</a>
N. Reimers and I. Gurevych, "Making monolingual sentence embeddings multilingual using knowledge distillation," Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020.
