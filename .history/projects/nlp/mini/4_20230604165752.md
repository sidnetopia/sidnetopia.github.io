## Word Embedding

**Project description:** Word embedding is a popular technique in natural language processing (NLP) that aims to represent words as dense vectors in a continuous vector space. These vector representations capture semantic and syntactic relationships between words, enabling machines to understand and process natural language more effectively. Word embedding has become an essential component in various NLP tasks, including text classification, machine translation, information retrieval, and sentiment analysis.

### Getting embeddings

```python
import pickle

import numpy as np
from gensim.models import KeyedVectors
from matplotlib import pyplot as plt
from sklearn.decomposition import PCA

from token_embedding import TokenEmbedding

def get_largest_indices(lst, k):
    k = -1 * k
    top_indices = np.argpartition(lst, k)[k:]

    return top_indices

def cos_sim(W, x, k):
    A = W
    B = x.reshape(-1,)
    A_norm = np.linalg.norm(A, 2)
    B_norm = np.linalg.norm(B, 2)
    print(A)
    print(B)
    print(A_norm)
    print(B_norm)
    cos = np.dot(A, B) / (A_norm * B_norm) + 1e-9 # avoid division by 0

    topk = get_largest_indices(cos, k)

    return topk, [cos[int(i)] for i in topk]

def get_similar_tokens(query_token, k, embed):
    if np.all(embed[[query_token]] == 0):
        raise "Division by zero"

    topk, cos = cos_sim(embed.idx_to_vec, embed[[query_token]], k + 1)

    for i, c in zip(topk[1:], cos[1:]):  # Exclude the input word
        print(f'{float(c):.3f}= {embed.idx_to_token[int(i)]}')

def get_analogy(token_a, token_b, token_c, k, embed):
    # token_a - token_b = token_c - token_d(?)
    # token_b - token_a + token_c = token_d(?)

    vecs = embed[[token_a, token_b, token_c]]
    x = vecs[1] - vecs[0] + vecs[2]
    topk, cos = cos_sim(embed.idx_to_vec, x, k)

    for i, c in zip(topk[1:], cos[1:]):  # Exclude the input word
        print(f'{float(c):.3f}: {embed.idx_to_token[int(i)]}')

def print_from_scratch(embedding, algorithm_title):
    print(f"From scratch word cosine similarity trained on {algorithm_title}")
    print("Word similarity:")
    for word in word_similarity_list:
        try:
            print(f"\nWord:{word}")
            get_similar_tokens(word, 10, embedding)
        except:
            print(f"Word {word} not found in embedding")
    print("")

    print("Word analogy:")
    for word in word_analogy_list:
        try:
            print(f"\n{word[0]} is to {word[1]}, as {word[2]} is to __: ")
            get_analogy(word[0], word[1], word[2], 10, embedding)
            print(f"answer: {word[3]}")
        except KeyError as e:
            print(f"Words {e.args} not found in embedding")

def print_from_gensim(embedding, algorithm_title):
    print(f"Using gensim library, word cosine similarity trained on {algorithm_title}")
    print("Word similarity:")
    word_similarities = {}
    for word in word_similarity_list:
        try:
            print(f"\nWord:{word}")
            sim = embedding.most_similar(positive=[word])
            print(sim)
            word_similarities[word] = sim
        except KeyError as e:
            print(f"Word {e.args[0]} not found in embedding")
    print("")

    print("Word analogy:")
    word_analogies = {}
    for word in word_analogy_list:
        try:
            print(f"\n{word[0]} is to {word[1]}, as {word[2]} is to __: ")
            word_analogy = embedding.most_similar(negative=[word[0]], positive=[word[1], word[2]])
            print(word_analogy)
            print(f"answer: {word[3]}")
            word_analogies[word[3]] = word_analogy
        except KeyError as e:
            print(f"Words {e.args} not found in embedding")
    file = open(f"word_sim_analogy.obj_{algorithm_title}", 'wb')
    pickle.dump({
        "word_similarities": word_similarities,
        "word_analogies":word_analogies
                 },
        file)

    return word_similarities, word_analogies

def plot_data(orig_data, labels):
    pca = PCA(n_components=2)
    data = pca.fit_transform(orig_data)
    plt.figure(figsize=(7, 5), dpi=100)
    plt.plot(data[:, 0], data[:, 1], '.')
    for i in range(len(data)):
        plt.annotate(labels[i], xy=data[i])
    for i in range(len(data) // 2):
        plt.annotate("",
                     xy=data[i],
                     xytext=data[i + len(data) // 2],
                     arrowprops=dict(arrowstyle="->",
                                     connectionstyle="arc3")
                     )



if __name__ == '__main__':
    embedding_dir = ''
    word_similarity_list = [
        "baba",
        "basa",
        "babae",
        "ako",
        "ospital",
        "hospital",
        "Marcos",
        "Piolo",
        "umaga",
        "kape",
    ]

    word_analogy_list = [
        ('umaga', 'breakfast', 'gabi', ''),  # near is to far, as open is to close
        ('lalaki', 'tatay', 'babae', ''),  # school is to teacher, as hospital is to doctor
        ('ospital', 'sakit', 'bahay', ''),  # hammer is to nail, as comb is to hair
        ('kape', 'mainit', 'kain', ''),  # hand write mouth read
        ('ulan', 'bagyo', 'araw', ''),  # dog is to puppy, as cat is to kitten
    ]

    # oscar_fasttext = TokenEmbedding(f'{embedding_dir}/oscar_w2v_fasttext')
    # oscar_word2vec = TokenEmbedding(f'{embedding_dir}/oscar_w2v_word2vec')
    # print_from_scratch(oscar_fasttext, 'fastText')
    # print("")
    # print_from_scratch(oscar_word2vec, 'word2vec')
    # print("")

    oscar_fasttext_gensim = KeyedVectors.load(f'{embedding_dir}/oscar_tl_fasttext.vec')
    oscar_word2vec_gensim = KeyedVectors.load(f'{embedding_dir}/oscar_tl_word2vec.vec')
    print_from_gensim(oscar_fasttext_gensim, 'fastText')
    print("")
    print_from_gensim(oscar_word2vec_gensim, 'word2vec')

```

#### Description

This code provides functionality for word embedding analysis using different algorithms. It includes functions for calculating word cosine similarity, finding similar tokens, and performing word analogies. It supports both a "from scratch" implementation and the usage of the `gensim` library.

#### Functions

get_largest_indices(lst, k)
This function returns the indices of the k largest elements in the input list lst.

#### cos_sim(W, x, k)

This function calculates the cosine similarity between a matrix W and a vector x using the dot product. It returns the indices and values of the k largest similarities.

#### get_similar_tokens(query_token, k, embed)

This function finds the k most similar tokens to the query_token in the given word embedding embed. It uses the cos_sim function to calculate cosine similarities.

#### get_analogy(token_a, token_b, token_c, k, embed)

This function performs word analogy by finding a token token_d such that token_a - token_b = token_c - token_d. It uses the cos_sim function to calculate cosine similarities.

#### print_from_scratch(embedding, algorithm_title)

This function prints word similarities and analogies using the "from scratch" implementation of word cosine similarity. It takes an embedding object and an algorithm_title as inputs.

#### print_from_gensim(embedding, algorithm_title)

This function prints word similarities and analogies using the gensim library. It takes an embedding object and an algorithm_title as inputs.

#### plot_data(orig_data, labels)

This function plots the data using Principal Component Analysis (PCA). It takes the original data and the corresponding labels as inputs.

#### Main Code

The main code demonstrates the usage of the functions. It loads word embeddings from the specified directory and calls the print_from_gensim function for both fastText and word2vec embeddings.

Please note that there are commented lines related to the "from scratch" implementation and the usage of a custom TokenEmbedding class, which are not used in the current code execution.

This code provides a way to analyze word embeddings and explore word similarities and analogies.

### Visualizing Embeddings

```python
import os
import pickle

import numpy as np
from gensim.models import KeyedVectors
from matplotlib import pyplot as plt
from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

def format_similarities(word_similarities, word_list):
    word_sim_dict = {}
    for word, similarities in word_similarities.items():
        try:
            key_idx = word_list.index(word)
            word_sim_dict[key_idx] = [word_list.index(word_sim[0]) for word_sim in similarities]
        except:
            word_sim_dict[word] = [word_list.index(word_sim[0]) for word_sim in similarities]

    return word_sim_dict

def format_word_analogies(word_analogies, word_list):
    word_analogy_list = [
        ['malapit', 'malayo', 'bukas', 'sarado'],  # near is to far, as open is to close
        ['paaralan', 'guro', 'ospital', 'doktor'],  # school is to teacher, as hospital is to doctor
        ['martilyo', 'pako', 'suklay', 'buhok'],  # hammer is to nail, as comb is to hair
        ['kamay', 'sulat', 'bibig', 'basa'],  # like is to love, as dislike is to hate
        ['aso', 'tuta', 'pusa', 'kuting'],  # dog is to puppy, as cat is to kitten
    ]
    word_analogy_list_new = []

    for word_analogy in word_analogy_list:
        temp = []
        for word in word_analogy:
            try:
                temp.append(word_list.index(word))
            except:
                temp.append(word)
        word_analogy_list_new.append(temp)

    for idx, (key, similarities) in enumerate(word_analogies.items()):
        if key == word_analogy_list[idx][3]:
            word_analogy_list_new[idx].append([word_list.index(word_sim[0]) for word_sim in similarities])

    return word_analogy_list_new

def plot_similarities(word_sims_idxs, points, word_list):
    fig, ax = plt.subplots(nrows=3, ncols=2, figsize=(15,15))
    fig.subplots_adjust(hspace=.15)
    subplot_list = []
    for row in ax:
        for col in row:
            subplot_list.append(col)

    for word_sim_dict_idx, (word, sims) in enumerate(word_sims_idxs.items()):
        new_X = []
        new_Y = []
        if isinstance(word, int):
            new_X.append(points[:, 0][word])
            new_Y.append(points[:, 1][word])
        else:
            new_X.append(None)
            new_Y.append(None)

        for idxs in sims:
            new_X.append(points[:, 0][idxs])
            new_Y.append(points[:, 1][idxs])

        ax = subplot_list[word_sim_dict_idx]
        ax.scatter(new_X[0], new_Y[0], color="red")
        ax.scatter(new_X[1:], new_Y[1:])
        # plt.xlabel("PC1", size=15)
        # plt.ylabel("PC2", size=15)
        ax.set_title(f" word embedding space for similarity of `{word_list[word]}`")

        ax.annotate(word_list[word], xy=(new_X[0], new_Y[0]), xytext=(0, 0), textcoords="offset points", color="red")
        for idx, sim in enumerate(sims):
            ax.annotate(word_list[sim], xy=(new_X[idx+1], new_Y[idx+1]), xytext=(0, 0), textcoords="offset points",
                         color="blue")

    plt.show()


def plot_analogies(word_analogies_idxs, points, word_list):
    fig, ax = plt.subplots(nrows=3, ncols=2, figsize=(15,15))
    fig.subplots_adjust(hspace=.5)
    subplot_list = []
    for row in ax:
        for col in row:
            subplot_list.append(col)

    for word_analogies_dict_idx, (word1, word2, word3, word4, sims) in enumerate(word_analogies_idxs):
        new_X = []
        new_Y = []
        if isinstance(word1, int):
            new_X.append(points[:, 0][word1])
            new_Y.append(points[:, 1][word1])
        else:
            new_X.append(None)
            new_Y.append(None)
        if isinstance(word2, int):
            new_X.append(points[:, 0][word2])
            new_Y.append(points[:, 1][word2])
        else:
            new_X.append(None)
            new_Y.append(None)
        if isinstance(word3, int):
            new_X.append(points[:, 0][word3])
            new_Y.append(points[:, 1][word3])
        else:
            new_X.append(None)
            new_Y.append(None)
        if isinstance(word4, int):
            new_X.append(points[:, 0][word4])
            new_Y.append(points[:, 1][word4])
        else:
            new_X.append(None)
            new_Y.append(None)

        for idxs in sims:
            new_X.append(points[:, 0][idxs])
            new_Y.append(points[:, 1][idxs])

        ax = subplot_list[word_analogies_dict_idx]
        ax.scatter(new_X[0], new_Y[0], color="red")
        ax.scatter(new_X[1], new_Y[1], color="purple")
        ax.scatter(new_X[2], new_Y[2], color="cyan")
        ax.scatter(new_X[3], new_Y[3], color="green")
        ax.scatter(new_X[4:], new_Y[4:])
        # plt.xlabel("PC1", size=15)
        # plt.ylabel("PC2", size=15)
        ax.set_title(f"word embedding space for analogy \n `{word_list[word1]} is to {word_list[word2]}, as {word_list[word3]} is to {word_list[word4]}`")

        ax.annotate(word_list[word1], xy=(new_X[0], new_Y[0]), xytext=(0, 0), textcoords="offset points", color="red")
        ax.annotate(word_list[word2], xy=(new_X[1], new_Y[1]), xytext=(0, 0), textcoords="offset points", color="purple")
        ax.annotate(word_list[word3], xy=(new_X[2], new_Y[2]), xytext=(0, 0), textcoords="offset points", color="cyan")
        ax.annotate(word_list[word4], xy=(new_X[3], new_Y[3]), xytext=(0, 0), textcoords="offset points", color="green")
        for idx, sim in enumerate(sims):
            ax.annotate(word_list[sim], xy=(new_X[idx], new_Y[idx]), xytext=(0, 0), textcoords="offset points",
                         color="blue")

    plt.show()

if __name__ == '__main__':
    embedding_dir = '/home/sidnetopia/Documents/nlp/assignments/word_embedding/embeddings/oscar_tl'
    model = KeyedVectors.load(f'{embedding_dir}/oscar_tl_fasttext.vec')

    pca = PCA(n_components=2, random_state=0)
    words = list(model.index_to_key)
    vectors = [model.get_vector(word).tolist() for word in words]
    Y = pca.fit_transform(vectors)

    filename_suffix = '_fastText'
    if os.path.isfile('word_sim_analogy_idxs.obj'):
        file = open(f"word_sim_analogy_idxs.obj", 'rb')
        dict_temp = pickle.load(file)
        word_sims_idxs = dict_temp['word_sims_idxs']
        word_analogies_idxs = dict_temp['word_analogies_idxs']
    else:
        file = open(f"word_sim_analogy.obj{filename_suffix}", 'rb')
        dict_temp = pickle.load(file)
        word_analogies = dict_temp['word_analogies']
        word_similarities = dict_temp['word_similarities']

        word_sims_idxs = format_similarities(word_similarities, words)
        word_analogies_idxs = format_word_analogies(word_analogies, words)

        file = open("word_sim_analogy_idxs.obj", 'wb')
        pickle.dump({
            "word_sims_idxs": word_sims_idxs,
            "word_analogies_idxs": word_analogies_idxs
                     },
            file)

    plot_similarities(word_sims_idxs, Y, words)
    plot_analogies(word_analogies_idxs, Y, words)
```

#### Description

The code provided is a Python script that performs various operations on word embeddings using the gensim library. It includes functions to format word similarities and analogies, as well as functions to plot the word embeddings in a two-dimensional space

Functions
format_similarities(word_similarities, word_list): This function takes a dictionary of word similarities and a list of words as input. It formats the word similarities by replacing the words with their corresponding indices in the word list. The function returns a dictionary where the keys are the indices of the words and the values are lists of indices representing similar words.

format_word_analogies(word_analogies, word_list): This function takes a dictionary of word analogies and a list of words as input. It formats the word analogies by replacing the words with their corresponding indices in the word list. The function returns a nested list where each sublist represents an analogy and contains the indices of the words involved.

plot_similarities(word_sims_idxs, points, word_list): This function takes a dictionary of word similarities, the coordinates of the word embeddings, and a list of words as input. It plots the word embeddings in a two-dimensional space and highlights the similarities between words. Each subplot represents a word similarity group, and the points are color-coded to indicate similarity.

plot_analogies(word_analogies_idxs, points, word_list): This function takes a list of word analogies, the coordinates of the word embeddings, and a list of words as input. It plots the word embeddings in a two-dimensional space and visualizes the analogies between words. Each subplot represents an analogy, and the points are color-coded to indicate the role of each word in the analogy.
